{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm environment\n",
    "GPUs can get lost when computer goes to sleep, requires restart\n",
    "Make sure that you're in a conda environment that supports Keras and has tensorflow-gpu installed\n",
    "\n",
    "potentially try: `alias gpureload=\"sudo rmmod nvidia_uvm ; sudo modprobe nvidia_uvm\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mritter/anaconda3/envs/tf_gpu_test04/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/mritter/anaconda3/envs/tf_gpu_test04:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "tensorflow-gpu            1.5.0                         0  \r\n",
      "tensorflow-gpu-base       1.5.0            py36h8a131e3_0  \r\n"
     ]
    }
   ],
   "source": [
    "! conda list tensorflow-gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF IT DOES NOT WORK, MAY NEED TO RESTART COMPUTER\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# confirm PyTorch sees the GPU\n",
    "from torch import cuda\n",
    "assert cuda.is_available()\n",
    "assert cuda.device_count() > 0\n",
    "print(cuda.get_device_name(cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://files.pushshift.io/hackernews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get  # to make GET request\n",
    "\n",
    "\n",
    "def download(url, file_name):\n",
    "    # open in binary mode\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url)\n",
    "        # write to file\n",
    "        file.write(response.content)\n",
    "\n",
    "download(BASE_URL+'HNI_total_items_by_month.txt', 'data/manifest.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        61 HNI_2006-10\r\n",
      "         1 HNI_2006-12\r\n",
      "      1549 HNI_2007-02\r\n",
      "      6305 HNI_2007-03\r\n",
      "     10335 HNI_2007-04\r\n",
      "      7516 HNI_2007-05\r\n",
      "      6036 HNI_2007-06\r\n",
      "      6410 HNI_2007-07\r\n",
      "     10841 HNI_2007-08\r\n",
      "     12371 HNI_2007-09\r\n"
     ]
    }
   ],
   "source": [
    "! head data/manifest.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134    [209738, HNI_2018-02]\n",
       "135    [237342, HNI_2018-03]\n",
       "136    [237609, HNI_2018-04]\n",
       "137    [237646, HNI_2018-05]\n",
       "138        [17172781, total]\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open('data/manifest.txt', 'r') as f:\n",
    "    manifest = pd.Series(f.read().split('\\n')).map(str.split)\n",
    "    manifest = manifest[manifest.map(len) > 0].reset_index(drop=True)\n",
    "manifest.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "DOWNLOADING TAKES A LONG TIME",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7016eb65f00f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DOWNLOADING TAKES A LONG TIME'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'{}.bz2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: DOWNLOADING TAKES A LONG TIME"
     ]
    }
   ],
   "source": [
    "# bulk_file_download.py\n",
    "import os.path\n",
    "\n",
    "assert 0, 'DOWNLOADING TAKES A LONG TIME'\n",
    "\n",
    "url_format = BASE_DIR+'{}.bz2'\n",
    "file_format = 'data/{}.bz2'\n",
    "for size, filename in manifest:\n",
    "    if os.path.isfile(file_format.format(filename)): continue \n",
    "    download(url_format.format(filename), file_format.format(filename))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mritter mritter   27M Jan 19 17:50 data/HNI_2016-11.bz2\r\n",
      "-rw-rw-r-- 1 mritter mritter   26M Jan 19 17:50 data/HNI_2016-12.bz2\r\n",
      "-rw-rw-r-- 1 mritter mritter   29M Jan 20 16:13 data/HNI_2017-01.bz2\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lah data/*bz2 | tail -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "import dask.bag as db\n",
    "from dask.distributed import Client, progress\n",
    "import json, re\n",
    "client = Client(n_workers=8, threads_per_worker=2, memory_limit='6GB')\n",
    "\n",
    "def comment_filter(record):\n",
    "    return (record['type'] == 'comment' \n",
    "            and record.get('deleted', None) == None\n",
    "            and record.get('text', None) != None)\n",
    "\n",
    "def text_transformation(record):\n",
    "    text = record['text'].lower()\n",
    "    text = re.sub('http.*\\w',' <LINK> ',text)\n",
    "    un = record['by'].lower()\n",
    "    return (un, text)\n",
    "\n",
    "b = db.read_text('data/*bz2').map(json.loads).filter(comment_filter)\\\n",
    "      .map(text_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "many &quot;dev bootcamps&quot; have been cropping up in the u.s. i wonder if that is &#x2f; will be a trend in europe?<p>also, hope you&#x27;re doing well morgante :) - fellow &#x27;13"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "i disagree. the c syntax is small and pretty easy to learn but the a learner will be stumped when it comes to installing the compiler, dealing with cryptic compiler errors, pointers and weak typing.<p>i&#x27;d recommend python as it&#x27;s easy to setup on any platform, has a clean easy to read syntax (newbies don&#x27;t need to worry about generators or decorators early on), has a repl and has a ton of good <i>free</i> learning references [1].<p>[1] - <a href=\" <LINK> >"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "in the uk, i&#x27;d get rather different numbers: the camry hybrid fuel tank is 70l, petrol is currently ~Â£1.20&#x2f;l, pound at 1.22 to the dollar (thanks brexit), so a full tank would be $102.48.<p>that gives a saving of $74&#x2f;680m or breakeven around the 50k miles point, if my maths is correct."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "CPU times: user 10.8 s, sys: 1.7 s, total: 12.5 s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "for row in b.random_sample(.0000001):  # 2m\n",
    "    display(HTML(row[1]))\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.07 s, sys: 1.8 s, total: 10.9 s\n",
      "Wall time: 1min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10763434"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "b.count().compute()  # 2m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "SAMPLE_LENGTH = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.77 s, sys: 755 ms, total: 5.53 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "non_target_texts = b.map(lambda x: x[1]).take(SAMPLE_LENGTH, npartitions=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.47 s, sys: 1.34 s, total: 9.82 s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "target_texts = b.filter(lambda x: x[0] == 'patio11').map(lambda x: x[1]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tuple(target_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = non_target_texts + tuple(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57663 unique tokens.\n",
      "Shape of data tensor: (59472, 500)\n",
      "Shape of label tensor: (59472, 2)\n",
      "CPU times: user 5.3 s, sys: 104 ms, total: 5.4 s\n",
      "Wall time: 5.16 s\n"
     ]
    }
   ],
   "source": [
    "# tokenize.py\n",
    "%%time\n",
    "tokenizer = Tokenizer(num_words=MAX_SEQUENCE_LENGTH)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray([0]*len(non_target_texts) + [1]*len(target_texts)))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 64 ms, total: 64 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/padded_data.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('dataset_1', data=data)\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.81 ms, sys: 0 ns, total: 3.81 ms\n",
      "Wall time: 3.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/labels.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('dataset_1', data=labels)\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47578, 500)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  17,   9,  20],\n",
       "       [  0,   0,   0, ..., 135,   2,  10],\n",
       "       [  0,   0,   0, ..., 104,  34, 118],\n",
       "       [  0,   0,   0, ...,  23,   3,   5],\n",
       "       [  0,   0,   0, ..., 375,  14,  64]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "CPU times: user 10.2 s, sys: 632 ms, total: 10.8 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "# create_embedding_matrix.py\n",
    "%%time\n",
    "# This is actually super fast\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "import os \n",
    "BASE_DIR = '/home/mritter/code/twitter_nlp/newsgroups_data/'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57663"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.2 ms, sys: 11.9 ms, total: 86.1 ms\n",
      "Wall time: 85.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# prepare embedding matrix\n",
    "\n",
    "num_distinct_words = len(tokenizer.word_index) + 1  # For <UNKNOWN> \n",
    "EMBEDDING_DIM = 100  # Dimensions to represent each token\n",
    "\n",
    "embedding_matrix = np.zeros((num_distinct_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > num_distinct_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57664, 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 87.5 ms, total: 87.5 ms\n",
      "Wall time: 85.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/whole_data.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('embedding_matrix', data=embedding_matrix)\n",
    "    h5f.create_dataset('x_train', data=x_train)\n",
    "    h5f.create_dataset('y_train', data=y_train)\n",
    "    h5f.create_dataset('x_val', data=x_val)\n",
    "    h5f.create_dataset('y_val', data=y_val)\n",
    "\n",
    "h5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "embedding_layer = Embedding(num_distinct_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190121-15-09'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "t = datetime.now()\n",
    "'{:%Y-%m-%d-%H-%M}'.format(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47578 samples, validate on 11894 samples\n",
      "Epoch 1/2\n",
      "47578/47578 [==============================] - 13s 281us/step - loss: 0.1537 - acc: 0.9533 - val_loss: 0.3822 - val_acc: 0.9058\n",
      "Epoch 2/2\n",
      "47578/47578 [==============================] - 13s 281us/step - loss: 0.1545 - acc: 0.9571 - val_loss: 0.3971 - val_acc: 0.9042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90b1469470>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TensorBoard instance with the path to the logs directory\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard as tb\n",
    "from datetime import datetime\n",
    "t = datetime.now()\n",
    "tensorboard = tb(log_dir='tensorboard_logs/{:%Y-%m-%d-%H-%M}'.format(t))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64, #128,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47578, 500)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prob_patio</th>\n",
       "      <th>original_index</th>\n",
       "      <th>original_text</th>\n",
       "      <th>is_patio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8147</td>\n",
       "      <td>0.82</td>\n",
       "      <td>30330</td>\n",
       "      <td>rtfa. the blog post lists a 2-3 word blurb for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5780</td>\n",
       "      <td>0.76</td>\n",
       "      <td>28511</td>\n",
       "      <td>you can get most of that info elsewhere, thoug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6093</td>\n",
       "      <td>0.76</td>\n",
       "      <td>7990</td>\n",
       "      <td>actual karma is defined as \"the total effect o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8374</td>\n",
       "      <td>0.74</td>\n",
       "      <td>26211</td>\n",
       "      <td>ah yes, but that is to buy military products w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3322</td>\n",
       "      <td>0.74</td>\n",
       "      <td>16147</td>\n",
       "      <td>low millions probably means low ones-of-millio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  prob_patio  original_index  \\\n",
       "0   8147        0.82           30330   \n",
       "1   5780        0.76           28511   \n",
       "2   6093        0.76            7990   \n",
       "3   8374        0.74           26211   \n",
       "4   3322        0.74           16147   \n",
       "\n",
       "                                       original_text  is_patio  \n",
       "0  rtfa. the blog post lists a 2-3 word blurb for...         0  \n",
       "1  you can get most of that info elsewhere, thoug...         0  \n",
       "2  actual karma is defined as \"the total effect o...         0  \n",
       "3  ah yes, but that is to buy military products w...         0  \n",
       "4  low millions probably means low ones-of-millio...         0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a feel for the outputs\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "samples = 10000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'prob_patio': model.predict(x_train[:samples])[:, 1].round(2),\n",
    "    'original_index': indices[:samples],\n",
    "    'original_text': [texts[i] for i in indices[:samples]],\n",
    "    'is_patio': labels[:samples,1].astype(int),\n",
    "})\n",
    "most_similar = df[df.is_patio == 0].sort_values('prob_patio', ascending=False).reset_index()\n",
    "most_similar.head()\n",
    "# loss_and_metrics = model.evaluate(x_train, y_train, batch_size=1)\n",
    "# print(y_train.mean(axis=0))\n",
    "# print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = most_similar.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "see also: \"google wants to do for radio what it did for the web\" and \"google wants to do for newspapers what it did for the web.\""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML((next(i)[1].original_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
