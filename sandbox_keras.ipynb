{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "* The big problem I'm trying to solve here is to make the relatively complex system of NNs for NLP something I cant 'wrap my head around'\n",
    "    * It's about learning, and one of the critical concepts in human learning is 'chunking'. So I've got three layers here: \n",
    "        * Top: \"string of text\" ==(NN system)==> username of interest [Y/N]\n",
    "        * Second: Collection of scripts with input/knobs/dials/outputs\n",
    "        * Third: Code itself, presented as close to linearly as possible\n",
    "        * Important to not try going too deep (I made this mistake with bash tools). Once you've got some solid abstractions that give you flexibility and don't seem to leak, extend rather than deepening knowledge\n",
    "    * Connections to concrete concepts that I already know are also key, so making the inputs/outputs clear in examples using Python datatypes that I'm comfortable with will help grasp the abstract transforms \n",
    "    * Reenforcement is key, so I'll try to copy this and apply it to a different problem almost as soon as I'm done\n",
    "    * The problem with many other systems is that, in an admirable effort to be modular, they force you to jump 3-7 levels deep within functions to get to the actual Python code (the concrete concept). From the main script, it can take several minutes to trace how an input and parameter actually get combined into an output.\n",
    "        * Even if effort was made to document, there's a \"curse of knowledge\" issue that makes it difficult to grasp the particulars without the mental model that the original writer had\n",
    "    * Goal is to find the mental models where I can look at other people's output (code), break it down quickly and accurately, identify what's new about it, and how i could update my design process to take advantage of anything that I don't yet have\n",
    "        * [This twitter thread](https://twitter.com/michael_nielsen/status/1074150124169773056) talks about meeting 'magicians' who are better in ways that you can't comprehend, then working to understand the implicit models that allow them to be 10x better. You probably can't do it just from their work, you need to communicate on a more abstract level. How do I do that here?\n",
    "    * Get solid on one simple approach, and then identify ways to extend it that I don't fully understand yet. Nailing down specific questions, \"I would have expected X, but I'm seeing Y - what's going on?\" is critical\n",
    "* There's huge value in having a single file where you can see everything, whether it's a diagram, makefile or Jupyter notebook\n",
    "* Need to identify what needs to be flexible. Passing a trivial amount of data e2e should be very doable. Then building on that, so that the data is essentially flowing (not moving in massive blocks)\n",
    "* From a documentation standpoint, unit tests waste too much time on the edge cases. At the very least, the top test should always be 'happy path' so you can see what it _should_ look like. Then there needs to be a connection between files\n",
    "    * This is much more possible for data pipelines than application code, so it's under-developed\n",
    "* One big thing that I think I lack is an intuitive sense of what's \"expensive\", in terms of Disk, IO, RAM and Compute (are there other limited resources?)\n",
    "    * Is streaming data from a server going to be a bottleneck?\n",
    "    * Is it computationally expensive to open a bunch of little data files, instead of one big one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# META\n",
    "###\n",
    "\n",
    "import h5py\n",
    "\n",
    "DATADIR = '/home/mritter/code/twitter_nlp/sandbox_data02/'\n",
    "NUM_SAMPLES = 10\n",
    "SKIP_FIRST = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNI_2006-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "HNI_2006-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "HNI_2007-02\n",
      "1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download_data.py\n",
    "\n",
    "# Input\n",
    "manifest_filename = 'manifest.txt'\n",
    "server_url = 'https://files.pushshift.io/hackernews/'\n",
    "\n",
    "# Knobs\n",
    "test_pct = .5\n",
    "\n",
    "# Output\n",
    "raw_data = 'raw_data'\n",
    "\n",
    "\n",
    "from requests import get\n",
    "import bz2, json, tqdm\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed = 42\n",
    "stop = False\n",
    "sample_l = []\n",
    "with open(DATADIR+manifest_filename) as infile:\n",
    "    for line in tqdm.tqdm(infile):  # Dial\n",
    "        remote_filename = line.split()[1]\n",
    "        print(remote_filename)\n",
    "        as_bytes = get(server_url+remote_filename+'.bz2').content\n",
    "        as_text = bz2.decompress(as_bytes)\n",
    "        for sample in as_text.split(b'\\n'):\n",
    "            if not len(sample): continue\n",
    "            sample_l.append(sample.decode(\"ascii\", \"ignore\"))\n",
    "            if len(sample_l) >= (SKIP_FIRST + NUM_SAMPLES):\n",
    "                stop = True\n",
    "            if stop: break\n",
    "        print(len(sample_l))\n",
    "        if stop: break\n",
    "            \n",
    "sample_l = sample_l[SKIP_FIRST:]\n",
    "np.random.shuffle(sample_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"by\":\"phil\",\"id\":1003,\"parent\":955,\"retrieved_on\":1525542114,\"text\":\"8.3% of what they did in 2005: wow.\",\"time\":1172399687,\"type\":\"comment\"}\n",
      "== shuffling ==\n",
      "{\"by\":\"rms\",\"descendants\":10,\"id\":1007,\"kids\":[1025],\"retrieved_on\":1525542114,\"score\":9,\"time\":1172404841,\"title\":\"What algorithm does news.YC use to filter spam?\",\"type\":\"story\"}\n"
     ]
    }
   ],
   "source": [
    "print(sample_l[0])\n",
    "print('== shuffling ==')\n",
    "np.random.seed = 42\n",
    "np.random.shuffle(sample_l)\n",
    "print(sample_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ix = int(len(sample_l)*test_pct)\n",
    "\n",
    "with open(DATADIR+raw_data+'_train.jsonl', 'w') as outfile:\n",
    "    outfile.write('\\n'.join(sample_l[test_ix:]))\n",
    "        \n",
    "with open(DATADIR+raw_data+'_test.jsonl', 'w') as outfile:\n",
    "    outfile.write('\\n'.join(sample_l[:test_ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mritter mritter 1016 Jan 26 20:59 sandbox_data/raw_data_test.jsonl\r\n",
      "-rw-rw-r-- 1 mritter mritter 2.1K Jan 26 20:59 sandbox_data/raw_data_train.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lah sandbox_data/*jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4 sandbox_data/raw_data_test.jsonl\r\n",
      "   4 sandbox_data/raw_data_train.jsonl\r\n",
      "   8 total\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l sandbox_data/*jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> sandbox_data/raw_data_test.jsonl <==\r\n",
      "{\"by\":\"phil\",\"id\":1003,\"parent\":955,\"retrieved_on\":1525542114,\"text\":\"8.3% of what they did in 2005: wow.\",\"time\":1172399687,\"type\":\"comment\"}\r\n",
      "{\"by\":\"python_kiss\",\"descendants\":0,\"id\":1002,\"retrieved_on\":1525542114,\"score\":2,\"time\":1172397259,\"title\":\"The Battle for Mobile Search\",\"type\":\"story\",\"url\":\"http:\\/\\/www.businessweek.com\\/technology\\/content\\/feb2007\\/tc20070220_828216.htm?campaign_id=rss_daily\"}\r\n",
      "{\"by\":\"volida\",\"id\":1010,\"parent\":856,\"retrieved_on\":1525542115,\"text\":\"Ebay bought its Chinese clone for hundreds of millions of dollars\",\"time\":1172413027,\"type\":\"comment\"}\r\n",
      "{\"by\":\"msgbeepa\",\"descendants\":0,\"id\":1008,\"retrieved_on\":1525542115,\"score\":1,\"time\":1172410393,\"title\":\"Great Way To Find New Job And Career\",\"type\":\"story\",\"url\":\"http:\\/\\/www.wikio.com\\/webinfo?id=13628228\"}\r\n",
      "{\"by\":\"python_kiss\",\"descendants\":0,\"id\":1001,\"retrieved_on\":1525542114,\"score\":3,\"time\":1172396128,\"title\":\"Wireless: India's Hot, China's Not\",\"type\":\"story\",\"url\":\"http:\\/\\/www.redherring.com\\/Article.aspx?a=21355\"}\r\n",
      "==> sandbox_data/raw_data_train.jsonl <==\r\n",
      "{\"by\":\"volida\",\"id\":1009,\"parent\":856,\"retrieved_on\":1525542115,\"text\":\"Ebay bought its Chinese clone for hundreds of millions of dollars, which afterwards collapsed because after moving the servers outside China the service's data were going through word filtering (e.g. during login) and there were failures...\\n\",\"time\":1172412920,\"type\":\"comment\"}\r\n",
      "{\"by\":\"rms\",\"descendants\":3,\"id\":1005,\"kids\":[1023,1067],\"retrieved_on\":1525542114,\"score\":6,\"time\":1172400839,\"title\":\"CRV Quickstart:   $250,000 in seed stage financing. How does an 18 year old entrepreneur find references to list on the application?\",\"type\":\"story\",\"url\":\"http:\\/\\/www.crv.com\\/AboutCRV\\/QuickStart.html\"}\r\n",
      "{\"by\":\"rms\",\"descendants\":10,\"id\":1007,\"kids\":[1025],\"retrieved_on\":1525542114,\"score\":9,\"time\":1172404841,\"title\":\"What algorithm does news.YC use to filter spam?\",\"type\":\"story\"}\r\n",
      "{\"by\":\"dngrmouse\",\"id\":1004,\"parent\":363,\"retrieved_on\":1525542114,\"text\":\"1. Have it so you can be automatically logged in. I have to manually log in every time I visit the site (using Safari here).<p>2. Just like Reddit does, show the domain each link belongs to. Reddit has this in brackets after the headline, which works fine. Since I don't have much free time, there are some sites that have sub-par content which I avoid reading, and it helps to know where I would end up without having to hover over the link.\",\"time\":1172400507,\"type\":\"comment\"}\r\n",
      "{\"by\":\"rms\",\"id\":1006,\"parent\":928,\"retrieved_on\":1525542114,\"text\":\"It's bad if you come out of the Techstars program without any funding and a non-sustainable company, but then you're probably screwed anyways. VCs are infamously inscrutable; we hear that they are always out to take advantage of naive or underfunded companies.<p>If you're good enough to get further investment after Techstars, you get it from a VC that you already know instead of having to deal with the typical painful negotiations. And if Brad Feld's Foundry Group will give you money, maybe you could get Bay Area VC money. Even better, the best companies will get to reinvest their own profits.\",\"time\":1172403958,\"type\":\"comment\"}"
     ]
    }
   ],
   "source": [
    "! head sandbox_data/*jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 5212.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "11\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess_txt.py\n",
    "\n",
    "# Input\n",
    "raw_data = 'raw_data_train.jsonl'\n",
    "\n",
    "# Knobs\n",
    "status = 'training'\n",
    "filter_bool = ('type', 'comment') \n",
    "split_regex = r' |\\.'\n",
    "remove_regex = r\"\\'|\\\"|,|\\.|\\n\"\n",
    "tag_patterns = {'http.*\\w':' <LINK> '}\n",
    "positive_labels = ('pg', 'patio11', 'volida')\n",
    "sequence_length = 300\n",
    "\n",
    "# Output\n",
    "preprocessed_data = 'preprocessed.txt'\n",
    "label_file = 'label_and_index.h5'\n",
    "\n",
    "import re\n",
    "\n",
    "labels = []\n",
    "original_ids = []\n",
    "\n",
    "with open(DATADIR+raw_data, 'r') as infile:\n",
    "    with open(DATADIR+preprocessed_data, 'w') as outfile:\n",
    "        for line in tqdm.tqdm(infile):  # Dial\n",
    "            line_json = json.loads(line)\n",
    "\n",
    "            if line_json[filter_bool[0]] != filter_bool[1]: continue\n",
    "            temp_text = line_json['text']\n",
    "            temp_text = temp_text.lower()\n",
    "            for key, value in tag_patterns.items():\n",
    "                temp_text = re.sub(key, value, temp_text)\n",
    "            text = re.split(split_regex, re.sub(remove_regex, '', temp_text))\n",
    "            print(len(text))\n",
    "            text += ['']*(sequence_length-len(text))\n",
    "            text += ['\\n']\n",
    "            outfile.write(','.join(text))\n",
    "            \n",
    "            if status == 'training':\n",
    "                labels.append((1, 0) if line_json['by'] in positive_labels else (0, 1))  # Not generalizable\n",
    "                original_ids.append(line_json['id'])\n",
    "            \n",
    "if status == 'training':\n",
    "    with h5py.File(DATADIR+label_file, \"w\") as f:\n",
    "        f.create_dataset('training_labels', (len(labels), 2), dtype='int', data=labels)\n",
    "        f.create_dataset('ordered_keys', (len(original_ids), 1), dtype='int', data=original_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebay,bought,its,chinese,clone,for,hundreds,of,millions,of,dollars,which,afterwards,collapsed,because,after,moving,the,servers,outside,china,the,services,data,were,going,through,word,filtering,(eg,during,login),and,there,were,failures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\r\n",
      "1,have,it,so,you,can,be,automatically,logged,in,i,have,to,manually,log,in,every,time,i,visit,the,site,(using,safari,here)<p>2,just,like,reddit,does,show,the,domain,each,link,belongs,to,reddit,has,this,in,brackets,after,the,headline,which,works,fine,since,i,dont,have,much,free,time,there,are,some,sites,that,have,sub-par,content,which,i,avoid,reading,and,it,helps,to,know,where,i,would,end,up,without,having,to,hover,over,the,link,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 sandbox_data/preprocessed.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_insight_: w2v is generated with a simple NN autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-26 22:00:00,869 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-01-26 22:00:00,870 : INFO : collecting all words and their counts\n",
      "2019-01-26 22:00:00,871 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-26 22:00:00,871 : INFO : collected 79 word types from a corpus of 102 raw words and 4 sentences\n",
      "2019-01-26 22:00:00,872 : INFO : Loading a fresh vocabulary\n",
      "2019-01-26 22:00:00,873 : INFO : min_count=1 retains 79 unique words (100% of original 79, drops 0)\n",
      "2019-01-26 22:00:00,873 : INFO : min_count=1 leaves 102 word corpus (100% of original 102, drops 0)\n",
      "2019-01-26 22:00:00,874 : INFO : deleting the raw counts dictionary of 79 items\n",
      "2019-01-26 22:00:00,875 : INFO : sample=0.001 downsamples 79 most-common words\n",
      "2019-01-26 22:00:00,875 : INFO : downsampling leaves estimated 35 word corpus (35.2% of prior 102)\n",
      "2019-01-26 22:00:00,876 : INFO : estimated required memory for 79 words and 2 dimensions: 40764 bytes\n",
      "2019-01-26 22:00:00,877 : INFO : resetting layer weights\n",
      "2019-01-26 22:00:00,878 : INFO : training model with 2 workers on 79 vocabulary and 2 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-01-26 22:00:00,882 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-26 22:00:00,883 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-26 22:00:00,883 : INFO : EPOCH - 1 : training on 102 raw words (40 effective words) took 0.0s, 30758 effective words/s\n",
      "2019-01-26 22:00:00,884 : INFO : training on a 102 raw words (40 effective words) took 0.0s, 8265 effective words/s\n",
      "2019-01-26 22:00:00,884 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# train_w2v.py\n",
    "\n",
    "# Input\n",
    "preprocessed_data = 'preprocessed.txt'\n",
    "\n",
    "#Knobs\n",
    "embedding_dim = 2\n",
    "\n",
    "# Output\n",
    "w2v_weights_and_word_index_mapping = 'w2v_weights_and_word_index_mapping.h5'\n",
    "\n",
    "class W2VIter:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    def __iter__(self):\n",
    "        for text in self.texts:\n",
    "            yield [token for token in text.split(',') if token != '']\n",
    "\n",
    "with open(DATADIR+preprocessed_data, 'r') as f:\n",
    "    w2viter = W2VIter(f.read().split('\\n'))\n",
    "\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "logger= logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "w2v = Word2Vec(w2viter, iter=1, min_count=1, size=embedding_dim, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1', 'have', 'it', 'so', 'you', 'can', 'be', 'automatically', 'logged', 'in', 'i', 'to', 'manually', 'log', 'every', 'time', 'visit', 'the', 'site', '(using', 'safari', 'here)<p>2', 'just', 'like', 'reddit', 'does', 'show', 'domain', 'each', 'link', 'belongs', 'has', 'this', 'brackets', 'after', 'headline', 'which', 'works', 'fine', 'since', 'dont', 'much', 'free', 'there', 'are', 'some', 'sites', 'that', 'sub-par', 'content', 'avoid', 'reading', 'and', 'helps', 'know', 'where', 'would', 'end', 'up', 'without', 'having', 'hover', 'over', 'ebay', 'bought', 'its', 'chinese', 'clone', 'for', 'hundreds', 'of', 'millions', 'dollars', '83%', 'what', 'they', 'did', '2005:', 'wow'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-26 22:00:00,979 : INFO : saving Word2Vec object under /home/mritter/code/twitter_nlp/sandbox_data02/myw2v.w2v, separately None\n",
      "2019-01-26 22:00:00,980 : INFO : not storing attribute vectors_norm\n",
      "2019-01-26 22:00:00,980 : INFO : not storing attribute cum_table\n",
      "2019-01-26 22:00:00,981 : INFO : saved /home/mritter/code/twitter_nlp/sandbox_data02/myw2v.w2v\n",
      "2019-01-26 22:00:00,982 : INFO : loading Word2Vec object from /home/mritter/code/twitter_nlp/sandbox_data02/myw2v.w2v\n",
      "2019-01-26 22:00:00,983 : INFO : loading wv recursively from /home/mritter/code/twitter_nlp/sandbox_data02/myw2v.w2v.wv.* with mmap=None\n",
      "2019-01-26 22:00:00,983 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-01-26 22:00:00,984 : INFO : loading vocabulary recursively from /home/mritter/code/twitter_nlp/sandbox_data02/myw2v.w2v.vocabulary.* with mmap=None\n",
      "2019-01-26 22:00:00,984 : INFO : loading trainables recursively from /home/mritter/code/twitter_nlp/sandbox_data02/myw2v.w2v.trainables.* with mmap=None\n",
      "2019-01-26 22:00:00,984 : INFO : setting ignored attribute cum_table to None\n",
      "2019-01-26 22:00:00,985 : INFO : loaded /home/mritter/code/twitter_nlp/sandbox_data02/myw2v.w2v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21018033 -0.18125333]\n",
      "i have in\n",
      "Index of \"you\" is: 13\n",
      "Index of \"you\" is: 13\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv['you'][:5])\n",
    "print(w2v.wv.index2word[0], w2v.wv.index2word[1], w2v.wv.index2word[2])\n",
    "print('Index of \"you\" is: {}'.format(w2v.wv.vocab['you'].index))\n",
    "w2v.save(DATADIR+\"myw2v.w2v\")\n",
    "w2v_loaded = Word2Vec.load(DATADIR+\"myw2v.w2v\")\n",
    "print('Index of \"you\" is: {}'.format(w2v_loaded.wv.vocab['you'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03932533,  0.18881728],\n",
       "       [ 0.1273874 , -0.08095149],\n",
       "       [-0.21662496,  0.07357214],\n",
       "       [-0.18276599, -0.16205968],\n",
       "       [-0.03800876,  0.00189891]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i, token in enumerate(w2v.wv.index2word): l.append(w2v.wv[token])\n",
    "weights = np.array(l)\n",
    "print(weights.shape)\n",
    "weights[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5964cbae80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(weights[:, 0], weights[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(DATADIR+w2v_weights_and_word_index_mapping, \"w\") as f:\n",
    "    f.create_dataset('weights', weights.shape, dtype='f', data=weights)\n",
    "    for word in w2v.wv.vocab.keys():\n",
    "        f.create_dataset('word_to_index/{}'.format(word), (1,), dtype='int', data=w2v.wv.vocab[word].index)\n",
    "    f.create_dataset('metadata/embedding_dim', (1,), dtype='int', data=embedding_dim)\n",
    "    f.create_dataset('metadata/max_token', (1,), dtype='int', data=weights.shape[0]+1)\n",
    "    f.create_dataset('metadata/sequence_length', (1,), dtype='int', data=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(using', '1', '2005:', '83%', 'after']\n",
      "[13]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(DATADIR+w2v_weights_and_word_index_mapping, \"r\") as f:\n",
    "    print(list(f['word_to_index'].keys())[:5])\n",
    "    print(f['word_to_index']['you'][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.  1.  6. 12. 13.]\n"
     ]
    }
   ],
   "source": [
    "# index_text.py\n",
    "\n",
    "# loading_dock: [local] train text, [local] gensim\n",
    "# shipping_dock: [local] text as indexes, [local] 100d w2v array sorted with that index\n",
    "# TODO should I bring the w2v sorting into here?\n",
    "# TODO I need to think about how IDs get passed around here\n",
    "\n",
    "# Input\n",
    "w2v_weights_and_word_index_mapping = 'w2v_weights_and_word_index_mapping.h5'\n",
    "preprocessed_data = 'preprocessed.txt'\n",
    "\n",
    "# Knobs\n",
    "pass\n",
    "\n",
    "# Output \n",
    "indexed_filename = 'indexed.h5'\n",
    "\n",
    "with h5py.File(DATADIR+w2v_weights_and_word_index_mapping, \"r\") as index:\n",
    "    with open(DATADIR+preprocessed_data, \"r\") as textfile:\n",
    "        \n",
    "        text_lines = textfile.read().split('\\n')\n",
    "        indexed = np.zeros(shape=(len(text_lines)-1, index['metadata']['sequence_length'].value[0]))\n",
    "        keys = index['word_to_index'].keys()\n",
    "        for line_ix, wordlist in enumerate(text_lines):\n",
    "            for word_ix, word in enumerate(wordlist.split(',')):\n",
    "                if word in keys:\n",
    "                    indexed[line_ix, word_ix] = index['word_to_index'][word][()]\n",
    "        \n",
    "    print(indexed[0, :5])\n",
    "\n",
    "    with h5py.File(DATADIR+indexed_filename, \"w\") as f:\n",
    "        f.create_dataset('training_data', \n",
    "                         indexed.shape, \n",
    "                         dtype='int', data=indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write outputs\n",
    "# with h5py.File(DATADIR+\"indexed.h5\", \"w\") as f:\n",
    "#     f.create_dataset('training_data', (len(indexed_texts), 300), dtype='int', data=list(indexed_texts.values()))\n",
    "# #     f.create_dataset('ordered_keys', (len(indexed_texts), 1), dtype='int', data=list(indexed_texts.keys()))\n",
    "# #     f.create_dataset('max_token', (1,), dtype='int', data=weights.shape[0]+1)\n",
    "# #     f.create_dataset('sequence_length', (1,), dtype='int', data=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73  5 74 75 76  2 77 78  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Try reading\n",
    "with h5py.File(DATADIR+indexed_filename, \"r\") as f:\n",
    "    print(f['training_data'][-1])\n",
    "#     print(f['ordered_keys'][:2])\n",
    "#     print(f['sequence_length'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 2)            160       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300, 32)           96        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 19202     \n",
      "=================================================================\n",
      "Total params: 19,458\n",
      "Trainable params: 19,298\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model.py\n",
    "\n",
    "# loading_dock: 100d w2v array, datafile\n",
    "# lever: params\n",
    "# dial: model.summary()\n",
    "# shipping_dock: [local] compiled model\n",
    "\n",
    "w2v_weights_and_word_index_mapping = 'w2v_weights_and_word_index_mapping.h5'\n",
    "training_file = 'indexed.h5'\n",
    "compiled_model = 'compiled_model.keras'\n",
    "\n",
    "# with h5py.File(DATADIR+training_file, \"r\") as f:\n",
    "#     max_token = f['max_token'][0]\n",
    "#     sequence_length = f['sequence_length'][0]\n",
    "\n",
    "with h5py.File(DATADIR+w2v_weights_and_word_index_mapping, \"r\") as f:\n",
    "    embedding_matrix = f['weights'][()]\n",
    "    embedding_dim = f['metadata/embedding_dim'][0]\n",
    "    num_distinct_words = f['metadata/max_token'][0]\n",
    "    sequence_length = f['metadata/sequence_length'][0]\n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Embedding, Flatten\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "sequence_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "embedded_sequences = Embedding(num_distinct_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(sequence_input)\n",
    "\n",
    "x = Dense(units=64, activation='relu')(embedded_sequences)\n",
    "x = Dense(units=32, activation='relu')(embedded_sequences)\n",
    "x = Flatten()(x)\n",
    "preds = Dense(units=2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.save(DATADIR+compiled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.6898 - acc: 0.6667\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6824 - acc: 0.6667\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "# loading_dock: [local] text as indexes, [local] compiled model\n",
    "# lever: epocs\n",
    "# dial: tensorboard\n",
    "# shipping_dock: [local] saved model\n",
    "\n",
    "training_file = 'indexed.h5'\n",
    "label_file = 'label_and_index.h5'\n",
    "\n",
    "compiled_model = 'compiled_model.keras'\n",
    "trained_model = 'trained_model.yaml'\n",
    "trained_weights = 'trained_weights.h5'\n",
    "\n",
    "from keras.callbacks import TensorBoard as tb\n",
    "from datetime import datetime\n",
    "t = datetime.now()\n",
    "tensorboard = tb(log_dir='tensorboard_logs/{:%Y-%m-%d-%H-%M}'.format(t))\n",
    "\n",
    "with h5py.File(DATADIR+training_file, \"r\") as f1:\n",
    "    with h5py.File(DATADIR+label_file, \"r\") as f2:\n",
    "        x_train = f1['training_data']  # Note that Keras is special in being able to read the HDF5 _object_\n",
    "        y_train = f2['training_labels']\n",
    "        \n",
    "        model.fit(x_train, y_train,\n",
    "                  batch_size=64, #128,\n",
    "                  epochs=2,\n",
    "                  shuffle='batch',  # Required for using HDF5\n",
    "#                   validation_data=(x_val, y_val),\n",
    "                  callbacks=[tensorboard])\n",
    "\n",
    "# serialize model to YAML\n",
    "# From https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "model_yaml = model.to_yaml()\n",
    "with open(DATADIR+trained_model, \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(DATADIR+trained_weights)\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_yaml\n",
    "# load YAML and create model\n",
    "yaml_file = open(DATADIR+trained_model, 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(DATADIR+trained_weights)\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.47513252, 0.5248675 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference.py\n",
    "\n",
    "# loading_dock: candidate comment\n",
    "# processing: call preprocess and index, then apply model\n",
    "# shipping_dock: best comment\n",
    "\n",
    "comment_text = \"\"\"\n",
    "Reminder, if you're in the US, the FTC says your eye doctor must give you your prescription after your exam. If a doctor refuses to do so, they can face legal action and penalties.\n",
    "\n",
    "https://www.consumer.ftc.gov/blog/2016/05/buying-prescriptio...\n",
    "\n",
    "That said, I don't think the FTC stipulates what information must appear on the prescription. Many docs leave off your PD (pupillary distance), which is a necessary measurement if you're buying online. Fortunately, there are a variety of easy ways to take this measurement yourself after the exam, although if you're really concerned about precision, you'll want the doctor's measurement.\n",
    "\n",
    "And by the way, it should go without saying, but I'll say it anyway. Although the quality of eyewear available online can be comparable to what you'd get in store ... please don't think an online eye exam is an acceptable substitute for visiting an ophthalmologist in person and getting a comprehensive eye exam! \n",
    "\"\"\"\n",
    "\n",
    "trained_model = 'trained_model.yaml'\n",
    "trained_weights = 'trained_weights.h5'\n",
    "\n",
    "\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "# load YAML and create model\n",
    "with open(DATADIR+trained_model, 'r') as yaml_file:\n",
    "    loaded_model_yaml = yaml_file.read()\n",
    "    loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(DATADIR+trained_weights)\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "\n",
    "indexed_texts = []\n",
    "for word in comment_text.lower().split():\n",
    "    if word in w2v.wv.vocab:\n",
    "        a = w2v.wv.vocab[word].index\n",
    "    else:\n",
    "        a = 0\n",
    "    indexed_texts.append(a)\n",
    "    \n",
    "indexed_texts += [0]*(sequence_length-len(indexed_texts))\n",
    "asarray = np.array([indexed_texts])\n",
    "loaded_model.predict(asarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "# loading_dock: [local] test file\n",
    "# processing: call inference, then compare to labels \n",
    "# shipping_dock: accuracy printout\n",
    "\n",
    "test_filename = 'downloaded_test'\n",
    "trained_model = 'trained_model'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
