{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "* The big problem I'm trying to solve here is to make the relatively complex system of NNs for NLP something I cant 'wrap my head around'\n",
    "    * It's about learning, and one of the critical concepts in human learning is 'chunking'. So I've got three layers here: \n",
    "        * Top: \"string of text\" ==(NN system)==> username of interest [Y/N]\n",
    "        * Second: Collection of scripts with input/knobs/dials/outputs\n",
    "        * Third: Code itself, presented as close to linearly as possible\n",
    "        * Important to not try going too deep (I made this mistake with bash tools). Once you've got some solid abstractions that give you flexibility and don't seem to leak, extend rather than deepening knowledge\n",
    "    * Connections to concrete concepts that I already know are also key, so making the inputs/outputs clear in examples using Python datatypes that I'm comfortable with will help grasp the abstract transforms \n",
    "    * Reenforcement is key, so I'll try to copy this and apply it to a different problem almost as soon as I'm done\n",
    "    * The problem with many other systems is that, in an admirable effort to be modular, they force you to jump 3-7 levels deep within functions to get to the actual Python code (the concrete concept). From the main script, it can take several minutes to trace how an input and parameter actually get combined into an output.\n",
    "        * Even if effort was made to document, there's a \"curse of knowledge\" issue that makes it difficult to grasp the particulars without the mental model that the original writer had\n",
    "    * Goal is to find the mental models where I can look at other people's output (code), break it down quickly and accurately, identify what's new about it, and how i could update my design process to take advantage of anything that I don't yet have\n",
    "        * [This twitter thread](https://twitter.com/michael_nielsen/status/1074150124169773056) talks about meeting 'magicians' who are better in ways that you can't comprehend, then working to understand the implicit models that allow them to be 10x better. You probably can't do it just from their work, you need to communicate on a more abstract level. How do I do that here?\n",
    "    * Get solid on one simple approach, and then identify ways to extend it that I don't fully understand yet. Nailing down specific questions, \"I would have expected X, but I'm seeing Y - what's going on?\" is critical\n",
    "* There's huge value in having a single file where you can see everything, whether it's a diagram, makefile or Jupyter notebook\n",
    "* Need to identify what needs to be flexible. Passing a trivial amount of data e2e should be very doable. Then building on that, so that the data is essentially flowing (not moving in massive blocks)\n",
    "* From a documentation standpoint, unit tests waste too much time on the edge cases. At the very least, the top test should always be 'happy path' so you can see what it _should_ look like. Then there needs to be a connection between files\n",
    "    * This is much more possible for data pipelines than application code, so it's under-developed\n",
    "* One big thing that I think I lack is an intuitive sense of what's \"expensive\", in terms of Disk, IO, RAM and Compute (are there other limited resources?)\n",
    "    * Is streaming data from a server going to be a bottleneck?\n",
    "    * Is it computationally expensive to open a bunch of little data files, instead of one big one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/home/mritter/code/twitter_nlp/sandbox_data/'\n",
    "NUM_SAMPLES = 10\n",
    "SKIP_FIRST = 1000\n",
    "test_pct = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNI_2006-10\n",
      "61\n",
      "HNI_2006-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "HNI_2007-02\n",
      "1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download_data.py\n",
    "\n",
    "# loading_dock: [local] sandbox_data.txt, [internet] http server\n",
    "# processing: download, assign IDs, split out test\n",
    "# dial: progress bar\n",
    "# shipping_dock: [local] .h5\n",
    "manifest_filename = 'manifest.txt'\n",
    "server_url = 'https://files.pushshift.io/hackernews/'\n",
    "output_file_base = 'downloaded'\n",
    "\n",
    "\n",
    "from requests import get\n",
    "import bz2, json, tqdm\n",
    "import numpy as np\n",
    "\n",
    "stop = False\n",
    "sample_l = []\n",
    "with open(DATADIR+manifest_filename) as infile:\n",
    "    for line in tqdm.tqdm(infile):\n",
    "        remote_filename = line.split()[1]\n",
    "        print(remote_filename)\n",
    "        as_bytes = get(server_url+remote_filename+'.bz2').content\n",
    "        as_text = bz2.decompress(as_bytes)\n",
    "        for sample in as_text.split(b'\\n'):\n",
    "            if not len(sample): continue\n",
    "            sample_l.append(json.loads(sample))\n",
    "            if len(sample_l) >= (SKIP_FIRST + NUM_SAMPLES):\n",
    "                stop = True\n",
    "            if stop: break\n",
    "        print(len(sample_l))\n",
    "        if stop: break\n",
    "            \n",
    "sample_l = sample_l[SKIP_FIRST:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "np.random.shuffle(sample_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ix = int(len(sample_l)*test_pct)\n",
    "\n",
    "with open(DATADIR+output_file_base+'_train.jsonl', 'w') as outfile:\n",
    "    for entry in sample_l[test_ix:]:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "with open(DATADIR+output_file_base+'_test.jsonl', 'w') as outfile:\n",
    "    for entry in sample_l[:test_ix]:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> sandbox_data/downloaded_test.jsonl <==\r\n",
      "{\"by\": \"volida\", \"id\": 1009, \"parent\": 856, \"retrieved_on\": 1525542115, \"text\": \"Ebay bought its Chinese clone for hundreds of millions of dollars, which afterwards collapsed because after moving the servers outside China the service's data were going through word filtering (e.g. during login) and there were failures...\\n\", \"time\": 1172412920, \"type\": \"comment\"}\r\n",
      "{\"by\": \"dngrmouse\", \"id\": 1004, \"parent\": 363, \"retrieved_on\": 1525542114, \"text\": \"1. Have it so you can be automatically logged in. I have to manually log in every time I visit the site (using Safari here).<p>2. Just like Reddit does, show the domain each link belongs to. Reddit has this in brackets after the headline, which works fine. Since I don't have much free time, there are some sites that have sub-par content which I avoid reading, and it helps to know where I would end up without having to hover over the link.\", \"time\": 1172400507, \"type\": \"comment\"}\r\n",
      "{\"by\": \"msgbeepa\", \"descendants\": 0, \"id\": 1008, \"retrieved_on\": 1525542115, \"score\": 1, \"time\": 1172410393, \"title\": \"Great Way To Find New Job And Career\", \"type\": \"story\", \"url\": \"http://www.wikio.com/webinfo?id=13628228\"}\r\n",
      "{\"by\": \"rms\", \"descendants\": 10, \"id\": 1007, \"kids\": [1025], \"retrieved_on\": 1525542114, \"score\": 9, \"time\": 1172404841, \"title\": \"What algorithm does news.YC use to filter spam?\", \"type\": \"story\"}\r\n",
      "{\"by\": \"phil\", \"id\": 1003, \"parent\": 955, \"retrieved_on\": 1525542114, \"text\": \"8.3% of what they did in 2005: wow.\", \"time\": 1172399687, \"type\": \"comment\"}\r\n",
      "\r\n",
      "==> sandbox_data/downloaded_train.jsonl <==\r\n",
      "{\"by\": \"volida\", \"id\": 1010, \"parent\": 856, \"retrieved_on\": 1525542115, \"text\": \"Ebay bought its Chinese clone for hundreds of millions of dollars\", \"time\": 1172413027, \"type\": \"comment\"}\r\n",
      "{\"by\": \"rms\", \"id\": 1006, \"parent\": 928, \"retrieved_on\": 1525542114, \"text\": \"It's bad if you come out of the Techstars program without any funding and a non-sustainable company, but then you're probably screwed anyways. VCs are infamously inscrutable; we hear that they are always out to take advantage of naive or underfunded companies.<p>If you're good enough to get further investment after Techstars, you get it from a VC that you already know instead of having to deal with the typical painful negotiations. And if Brad Feld's Foundry Group will give you money, maybe you could get Bay Area VC money. Even better, the best companies will get to reinvest their own profits.\", \"time\": 1172403958, \"type\": \"comment\"}\r\n",
      "{\"by\": \"python_kiss\", \"descendants\": 0, \"id\": 1002, \"retrieved_on\": 1525542114, \"score\": 2, \"time\": 1172397259, \"title\": \"The Battle for Mobile Search\", \"type\": \"story\", \"url\": \"http://www.businessweek.com/technology/content/feb2007/tc20070220_828216.htm?campaign_id=rss_daily\"}\r\n",
      "{\"by\": \"rms\", \"descendants\": 3, \"id\": 1005, \"kids\": [1023, 1067], \"retrieved_on\": 1525542114, \"score\": 6, \"time\": 1172400839, \"title\": \"CRV Quickstart:   $250,000 in seed stage financing. How does an 18 year old entrepreneur find references to list on the application?\", \"type\": \"story\", \"url\": \"http://www.crv.com/AboutCRV/QuickStart.html\"}\r\n",
      "{\"by\": \"python_kiss\", \"descendants\": 0, \"id\": 1001, \"retrieved_on\": 1525542114, \"score\": 3, \"time\": 1172396128, \"title\": \"Wireless: India's Hot, China's Not\", \"type\": \"story\", \"url\": \"http://www.redherring.com/Article.aspx?a=21355\"}\r\n"
     ]
    }
   ],
   "source": [
    "! head sandbox_data/*jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 14463.12it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 4422.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'by': 'volida', 'id': 1010, 'parent': 856, 'retrieved_on': 1525542115, 'text': 'Ebay bought its Chinese clone for hundreds of millions of dollars', 'time': 1172413027, 'type': 'comment'}, {'by': 'rms', 'id': 1006, 'parent': 928, 'retrieved_on': 1525542114, 'text': \"It's bad if you come out of the Techstars program without any funding and a non-sustainable company, but then you're probably screwed anyways. VCs are infamously inscrutable; we hear that they are always out to take advantage of naive or underfunded companies.<p>If you're good enough to get further investment after Techstars, you get it from a VC that you already know instead of having to deal with the typical painful negotiations. And if Brad Feld's Foundry Group will give you money, maybe you could get Bay Area VC money. Even better, the best companies will get to reinvest their own profits.\", 'time': 1172403958, 'type': 'comment'}]\n",
      "{1010: ['ebay', 'bought', 'its', 'chinese', 'clone', 'for', 'hundreds', 'of', 'millions', 'of', 'dollars', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 1006: ['its', 'bad', 'if', 'you', 'come', 'out', 'of', 'the', 'techstars', 'program', 'without', 'any', 'funding', 'and', 'a', 'non-sustainable', 'company,', 'but', 'then', 'youre', 'probably', 'screwed', 'anyways', '', 'vcs', 'are', 'infamously', 'inscrutable;', 'we', 'hear', 'that', 'they', 'are', 'always', 'out', 'to', 'take', 'advantage', 'of', 'naive', 'or', 'underfunded', 'companies', '<p>if', 'youre', 'good', 'enough', 'to', 'get', 'further', 'investment', 'after', 'techstars,', 'you', 'get', 'it', 'from', 'a', 'vc', 'that', 'you', 'already', 'know', 'instead', 'of', 'having', 'to', 'deal', 'with', 'the', 'typical', 'painful', 'negotiations', '', 'and', 'if', 'brad', 'felds', 'foundry', 'group', 'will', 'give', 'you', 'money,', 'maybe', 'you', 'could', 'get', 'bay', 'area', 'vc', 'money', '', 'even', 'better,', 'the', 'best', 'companies', 'will', 'get', 'to', 'reinvest', 'their', 'own', 'profits', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']}\n",
      "{1010: (1, 0), 1006: (0, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess_data.py\n",
    "\n",
    "# loading_dock: [local] train data\n",
    "# processing: filter, split, tag, format labels\n",
    "# lever: training, inference, evaluation\n",
    "# dial: Dask status\n",
    "# shipping_dock: [local] text-only and label-only files with IDs\n",
    "status = 'training'\n",
    "downloaded_filename = 'downloaded'\n",
    "filter_bools = {'type':'story'}  # Lines are filtered out if true\n",
    "split_regex = r' |\\.'\n",
    "remove_regex = r\"\\'|\\\"\"\n",
    "tag_patterns = {'http.*\\w':' <LINK> '}\n",
    "positive_labels = ('pg', 'patio11', 'volida')\n",
    "output_file = 'preprocessed'\n",
    "\n",
    "import re\n",
    "\n",
    "data = []\n",
    "with open(DATADIR+downloaded_filename+'_train.jsonl', 'r') as infile:\n",
    "    for line in tqdm.tqdm(infile):\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "for key, value in filter_bools.items():\n",
    "    data = [x for x in data if x[key] != value]\n",
    "\n",
    "texts = {}\n",
    "labels = {}\n",
    "temp_labels = []\n",
    "for row in tqdm.tqdm(data):\n",
    "    temp_text = row['text']\n",
    "    temp_text = temp_text.lower()\n",
    "    for key, value in tag_patterns.items():\n",
    "        temp_text = re.sub(key, value, temp_text)\n",
    "    texts[row['id']] = re.split(split_regex, re.sub(remove_regex, '', temp_text))\n",
    "    labels[row['id']] = (1, 0) if row['by'] in positive_labels else (0, 1)  # Not generalizable\n",
    "\n",
    "for value in texts.values():\n",
    "    value += ['']*(300-len(value))\n",
    "print(data)\n",
    "print(texts)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "with h5py.File(DATADIR+\"preprocessed_text.h5\", \"w\") as f:\n",
    "    for ix in texts.keys():\n",
    "        dset = f.create_dataset(str(ix), (300,), dtype='S100')\n",
    "        dset[:] = [str(n).encode(\"ascii\", \"ignore\") for n in texts[ix]]\n",
    "\n",
    "with h5py.File(DATADIR+\"preprocessed_label.h5\", \"w\") as f:\n",
    "    for ix in labels.keys():\n",
    "        dset = f.create_dataset(str(ix), (2,), dtype='i')\n",
    "        dset[:] = labels[ix]\n",
    "\n",
    "with h5py.File(DATADIR+\"preprocessed_label.h5\", \"r\") as f:\n",
    "    print(f['1010'][()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_insight_: w2v is generated with a simple NN autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 06:47:16,018 : INFO : collecting all words and their counts\n",
      "2019-01-24 06:47:16,019 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-24 06:47:16,020 : INFO : collected 86 word types from a corpus of 113 raw words and 2 sentences\n",
      "2019-01-24 06:47:16,020 : INFO : Loading a fresh vocabulary\n",
      "2019-01-24 06:47:16,021 : INFO : min_count=1 retains 86 unique words (100% of original 86, drops 0)\n",
      "2019-01-24 06:47:16,021 : INFO : min_count=1 leaves 113 word corpus (100% of original 113, drops 0)\n",
      "2019-01-24 06:47:16,022 : INFO : deleting the raw counts dictionary of 86 items\n",
      "2019-01-24 06:47:16,023 : INFO : sample=0.001 downsamples 86 most-common words\n",
      "2019-01-24 06:47:16,023 : INFO : downsampling leaves estimated 41 word corpus (37.1% of prior 113)\n",
      "2019-01-24 06:47:16,023 : INFO : estimated required memory for 86 words and 100 dimensions: 111800 bytes\n",
      "2019-01-24 06:47:16,024 : INFO : resetting layer weights\n",
      "2019-01-24 06:47:16,028 : INFO : training model with 2 workers on 86 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-01-24 06:47:16,030 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-24 06:47:16,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-24 06:47:16,031 : INFO : EPOCH - 1 : training on 113 raw words (42 effective words) took 0.0s, 34860 effective words/s\n",
      "2019-01-24 06:47:16,033 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-24 06:47:16,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-24 06:47:16,034 : INFO : EPOCH - 2 : training on 113 raw words (38 effective words) took 0.0s, 35584 effective words/s\n",
      "2019-01-24 06:47:16,035 : INFO : training on a 226 raw words (80 effective words) took 0.0s, 13693 effective words/s\n",
      "2019-01-24 06:47:16,035 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# train_w2v.py\n",
    "\n",
    "# loading_dock: [local] train text\n",
    "# processing: gensim w2v\n",
    "# dial: estimate based on train size\n",
    "# shipping_dock: [local] gensim model\n",
    "train_filename = 'preprocessed'\n",
    "output_file = 'w2v'\n",
    "\n",
    "class W2VIter:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts.values()\n",
    "    def __iter__(self):\n",
    "        for text in self.texts:\n",
    "            yield [token for token in text if token != '']\n",
    "            \n",
    "w2viter = W2VIter(texts)\n",
    "\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "logger= logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# TODO This doesn't work yet\n",
    "class EpochFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.getMessage().contains('EPOCH')\n",
    "\n",
    "logger.addFilter(EpochFilter())\n",
    "\n",
    "w2v = Word2Vec(w2viter, iter=2, min_count=1, size=100, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 06:47:20,973 : INFO : saving Word2Vec object under /home/mritter/code/twitter_nlp/sandbox_data/myw2v, separately None\n",
      "2019-01-24 06:47:20,974 : INFO : not storing attribute vectors_norm\n",
      "2019-01-24 06:47:20,974 : INFO : not storing attribute cum_table\n",
      "2019-01-24 06:47:20,976 : INFO : saved /home/mritter/code/twitter_nlp/sandbox_data/myw2v\n",
      "2019-01-24 06:47:20,977 : INFO : loading Word2Vec object from /home/mritter/code/twitter_nlp/sandbox_data/myw2v\n",
      "2019-01-24 06:47:20,979 : INFO : loading wv recursively from /home/mritter/code/twitter_nlp/sandbox_data/myw2v.wv.* with mmap=None\n",
      "2019-01-24 06:47:20,979 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-01-24 06:47:20,980 : INFO : loading vocabulary recursively from /home/mritter/code/twitter_nlp/sandbox_data/myw2v.vocabulary.* with mmap=None\n",
      "2019-01-24 06:47:20,980 : INFO : loading trainables recursively from /home/mritter/code/twitter_nlp/sandbox_data/myw2v.trainables.* with mmap=None\n",
      "2019-01-24 06:47:20,981 : INFO : setting ignored attribute cum_table to None\n",
      "2019-01-24 06:47:20,981 : INFO : loaded /home/mritter/code/twitter_nlp/sandbox_data/myw2v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00214999  0.00413239  0.00452458  0.00431084 -0.0031492 ]\n",
      "of you to\n",
      "Index of \"the\" is: 4\n",
      "Index of \"the\" is: 4\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv['the'][:5])\n",
    "print(w2v.wv.index2word[0], w2v.wv.index2word[1], w2v.wv.index2word[2])\n",
    "print('Index of \"the\" is: {}'.format(w2v.wv.vocab['the'].index))\n",
    "w2v.save(DATADIR+\"myw2v\")\n",
    "w2v_loaded = Word2Vec.load(DATADIR+\"myw2v\")\n",
    "print('Index of \"the\" is: {}'.format(w2v_loaded.wv.vocab['the'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00060824,  0.0011704 , -0.00019772, -0.0045374 , -0.00032814],\n",
       "       [-0.00120165, -0.00185802,  0.00117371,  0.00338027, -0.0031534 ],\n",
       "       [ 0.00209898, -0.0028773 , -0.00276368, -0.00330904, -0.00310643],\n",
       "       [-0.00444596, -0.00370529, -0.00366363,  0.00253816,  0.00022051],\n",
       "       [ 0.00214999,  0.00413239,  0.00452458,  0.00431084, -0.0031492 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i, token in enumerate(w2v.wv.index2word): l.append(w2v.wv[token])\n",
    "weights = np.array(l)\n",
    "weights[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DATADIR+\"w2v.h5\", \"w\") as f:\n",
    "    dset = f.create_dataset('data', weights.shape, dtype='f')\n",
    "    dset[:] = weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 07:19:15,687 : INFO : loading Word2Vec object from /home/mritter/code/twitter_nlp/sandbox_data/myw2v\n",
      "2019-01-24 07:19:15,689 : INFO : loading wv recursively from /home/mritter/code/twitter_nlp/sandbox_data/myw2v.wv.* with mmap=None\n",
      "2019-01-24 07:19:15,690 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-01-24 07:19:15,691 : INFO : loading vocabulary recursively from /home/mritter/code/twitter_nlp/sandbox_data/myw2v.vocabulary.* with mmap=None\n",
      "2019-01-24 07:19:15,691 : INFO : loading trainables recursively from /home/mritter/code/twitter_nlp/sandbox_data/myw2v.trainables.* with mmap=None\n",
      "2019-01-24 07:19:15,692 : INFO : setting ignored attribute cum_table to None\n",
      "2019-01-24 07:19:15,692 : INFO : loaded /home/mritter/code/twitter_nlp/sandbox_data/myw2v\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16, 17, 5, 18, 19, 20, 21, 0, 22, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index_text.py\n",
    "\n",
    "# loading_dock: [local] train text, [local] gensim\n",
    "# shipping_dock: [local] text as indexes, [local] 100d w2v array sorted with that index\n",
    "# TODO should I bring the w2v sorting into here?\n",
    "# TODO I need to think about how IDs get passed around here\n",
    "w2v_file = 'w2v'\n",
    "text_file = 'preprocessed'\n",
    "\n",
    "w2v_loaded = Word2Vec.load(DATADIR+\"myw2v\")\n",
    "indexed_texts = {}\n",
    "for key, wordlist in texts.items():\n",
    "    indexed_texts[key] = []\n",
    "    for word in wordlist:\n",
    "        if word in w2v_loaded.wv.vocab:\n",
    "            a = w2v_loaded.wv.vocab[word].index\n",
    "        else:\n",
    "            a = 0\n",
    "        indexed_texts[key].append(a)\n",
    "indexed_texts[1010][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "# loading_dock: 100d w2v array\n",
    "# lever: params\n",
    "# dial: model.summary()\n",
    "# shipping_dock: [local] compiled model\n",
    "\n",
    "w2v_file = 'w2v'\n",
    "compiled_model = 'compiled_model'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "# loading_dock: [local] text as indexes, [local] compiled model\n",
    "# lever: epocs\n",
    "# dial: tensorboard\n",
    "# shipping_dock: [local] saved model\n",
    "\n",
    "text_file = 'preprocessed'\n",
    "compiled_model = 'compiled_model'\n",
    "trained_model = 'trained_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference.py\n",
    "\n",
    "# loading_dock: candidate comment\n",
    "# processing: call preprocess and index, then apply model\n",
    "# shipping_dock: best comment\n",
    "\n",
    "comment_text = \"\"\"\n",
    "Reminder, if you're in the US, the FTC says your eye doctor must give you your prescription after your exam. If a doctor refuses to do so, they can face legal action and penalties.\n",
    "\n",
    "https://www.consumer.ftc.gov/blog/2016/05/buying-prescriptio...\n",
    "\n",
    "That said, I don't think the FTC stipulates what information must appear on the prescription. Many docs leave off your PD (pupillary distance), which is a necessary measurement if you're buying online. Fortunately, there are a variety of easy ways to take this measurement yourself after the exam, although if you're really concerned about precision, you'll want the doctor's measurement.\n",
    "\n",
    "And by the way, it should go without saying, but I'll say it anyway. Although the quality of eyewear available online can be comparable to what you'd get in store ... please don't think an online eye exam is an acceptable substitute for visiting an ophthalmologist in person and getting a comprehensive eye exam! \n",
    "\"\"\"\n",
    "\n",
    "trained_model = 'trained_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "# loading_dock: [local] test file\n",
    "# processing: call inference, then compare to labels \n",
    "# shipping_dock: accuracy printout\n",
    "\n",
    "test_filename = 'downloaded_test'\n",
    "trained_model = 'trained_model'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
